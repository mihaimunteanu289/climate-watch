{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "632bce07-d56a-4c44-9085-9fe20c881ed6",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23bd0ab1-c07d-4782-96ad-c4910514822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unicodedata\n",
    "import json\n",
    "import ast\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155fbcab-2138-4d2e-9674-74b68bc22517",
   "metadata": {},
   "source": [
    "### Read dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2546943-78a1-4e5f-b115-c4dd827483a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/datasets/dataset_cleaned.csv\")\n",
    "\n",
    "# Convert the string representation to an actual list\n",
    "df['phenomena'] = df['phenomena'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a635faf8-b809-42da-8ef9-a9835cf562af",
   "metadata": {},
   "source": [
    "### Remove rows where phenoma is not list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbbbc47f-bf8a-4bd2-af0b-4347690d1cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows where the 'phenomena' column does not contain a list of strings\n",
    "df = df[df['phenomena'].apply(lambda x: isinstance(x, list) and all(isinstance(item, str) for item in x))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed2c2c6-ccbc-4785-88f8-b00a78dfa60d",
   "metadata": {},
   "source": [
    "### Remove diacritics and lowercase all the elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44109374-1e8d-41ca-b147-1d8d8e4cd4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_diacritics(input_str):\n",
    "    \"\"\"\n",
    "    Removes diacritics from a given string.\n",
    "    \"\"\"\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    return ''.join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
    "\n",
    "def lowercase_diacritics_remove(text):\n",
    "    \"\"\"\n",
    "    Lowercase and remove diacritics for each word in the list.\n",
    "    \"\"\"\n",
    "    # Ensure that the input is a list\n",
    "    if not isinstance(text, list):\n",
    "        raise ValueError(f\"Expected a list, but got {type(text)}\")\n",
    "\n",
    "    processed_list = [remove_diacritics(word.lower()) for word in text]\n",
    "    return processed_list\n",
    "\n",
    "# Apply the function to the 'phenomena' column\n",
    "df['clean_processed_phenomena'] = df['phenomena'].apply(lowercase_diacritics_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9d8c76-dee4-4e86-beaa-50b26f8b3ca7",
   "metadata": {},
   "source": [
    "### Remove uncommon phenomena from our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98278af9-2419-4201-bea9-e809497664b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_list = [word for sublist in df['clean_processed_phenomena'] for word in sublist]\n",
    "word_counts = Counter(flattened_list)\n",
    "valid_words = {word for word, count in word_counts.items() if count >= 10}\n",
    "\n",
    "mask = df['clean_processed_phenomena'].apply(lambda x: all(word in valid_words for word in x))\n",
    "df = df[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf67706a-381f-4d02-9b27-c05578520e8c",
   "metadata": {},
   "source": [
    "### Count all the occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bc631f9-5426-428b-ad8a-2c54a4216804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the lists in the 'clean_processed_phenomena' column\n",
    "flattened_list = [word for sublist in df['clean_processed_phenomena'] for word in sublist]\n",
    "\n",
    "# Use Counter to get the counts of each string\n",
    "word_counts = Counter(flattened_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f8c286-807d-42ea-bfa9-e80eb669f537",
   "metadata": {},
   "source": [
    "### Map the related events together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18fa4095-f13b-49e6-b7a7-2009b7628bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary to JSON\n",
    "json_filename = \"mappings/phenomena_mappings.json\"\n",
    "\n",
    "# Read back the JSON as a dictionary\n",
    "with open(json_filename, \"r\") as json_file:\n",
    "    phenomena_mappings = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f763c975-f1e4-4f96-9712-a57ea7e18438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_terms(phenomena_list):\n",
    "    return [phenomena_mappings.get(phenomenon, phenomenon) for phenomenon in phenomena_list]\n",
    "\n",
    "df['clean_processed_phenomena'] = [consolidate_terms(phenomena) for phenomena in df['clean_processed_phenomena']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169c9305-8956-4982-9829-02658027f2d9",
   "metadata": {},
   "source": [
    "### Remove non-event related words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ed19fd5-57f5-46f6-8af1-f4b9ca885704",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_meteorological_words = [\n",
    "    'alunecare', 'cer variabil', 'cer senin', 'cer acoperit', \n",
    "    'cod portocaliu', 'cod galben', 'cutremur', 'cer noros', \n",
    "    'eroziune', 'gheata', 'ghetus', 'instabilitate', 'insorit', 'mixt',\n",
    "    'mixta', 'posibil grindina', 'posibil vijelii', 'soare', 'vreme frumoasa',\n",
    "    'nori', 'vreme schimbatoare', 'vreme inchisa', 'vreme insorita', 'temperaturi in crestere',\n",
    "    'incalzire', 'racire', 'racire accentuata'\n",
    "]\n",
    "\n",
    "# Remove the non-meteorological words from the 'clean_processed_phenomena' column\n",
    "df['clean_processed_phenomena'] = df['clean_processed_phenomena'].apply(lambda x: [word for word in x if word not in non_meteorological_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34641fc6-f5e0-4860-b610-71bdabf01c0c",
   "metadata": {},
   "source": [
    "### Translate events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "617d3ec4-3c0b-4421-8f74-0c54d145d09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translation dictionary\n",
    "translation_dict = {\n",
    "    'ninsori': 'snowfalls',\n",
    "    'lapovita': 'sleet',\n",
    "    'ploi': 'rain',\n",
    "    'vant': 'wind',\n",
    "    'fulguieli': 'lightning',\n",
    "    'inundatie': 'flood',\n",
    "    'polei': 'glaze ice',\n",
    "    'ger': 'frost',\n",
    "    'ceata': 'fog',\n",
    "    'canicula': 'heatwave',\n",
    "    'negura': 'mist',\n",
    "    'descarcari electrice': 'electrical discharges',\n",
    "    'cald': 'warmth',\n",
    "    'averse': 'showers',\n",
    "    'frig': 'cold',\n",
    "    'furtuna': 'storm',\n",
    "    'grindina': 'hail',\n",
    "    'vijelie': 'gust',\n",
    "    'innorari': 'cloudiness',\n",
    "    'avalansa': 'avalanche',\n",
    "    'burnita': 'drizzle',\n",
    "    'viscol': 'blizzard',\n",
    "    'tunet': 'electrical discharges',\n",
    "    'bruma': 'hoarfrost',\n",
    "    'chiciura': 'rime',\n",
    "    'seceta': 'drought',\n",
    "    'instabilitate atmosferica': 'atmospheric instability',\n",
    "    'ciclon': 'cyclone',\n",
    "    'inundatii': 'floods',\n",
    "    'incendii': 'fires',\n",
    "    'disconfort termic': 'thermal discomfort',\n",
    "    'uragan': 'hurricane',\n",
    "    'tornada': 'tornado',\n",
    "    'viitura': 'flash flood'\n",
    "}\n",
    "\n",
    "df['clean_processed_phenomena'] = df['clean_processed_phenomena'].apply(lambda x: [translation_dict.get(word, word) for word in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33923083-f126-40a1-9e7f-e6ac40afd0f7",
   "metadata": {},
   "source": [
    "### Remove rows where list is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba2bdb01-e468-4335-b212-51378715455b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['clean_processed_phenomena'].astype(bool)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e59520-0f3e-43d2-bd1f-2d0c6ffdca19",
   "metadata": {},
   "source": [
    "### Export dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b1a36d2-afe6-478d-94a8-b9b23116a3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/datasets/dataset_phenomena_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae88182-b1a2-4e9e-a249-91748b80393b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
